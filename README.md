# ìœ íŠœë¸Œ ì˜ìƒ íƒœê·¸, ë°˜ì‘ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±ê¸°

## https://github.com/sdouf5054/20252R0136COSE36203 ìµœì¢… íŒŒì´í”„ë¼ì¸

## ì¶”ê°€ bert_model.py YouTube_Comment_Sentiment_Unified.ipynbì˜ ë™ì¼í•œ ê¸°ëŠ¥ì„ ê°€ì§„ py ìŠ¤í¬ë¦½íŠ¸

# ğŸ“ Repository Structure
# 1. Jupyter Notebook
YouTube_Comment_Sentiment_Unified.ipynb

ì „ì²´ í”„ë¡œì íŠ¸ë¥¼ í†µí•©í•´ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ ë…¸íŠ¸ë¶ì…ë‹ˆë‹¤.

### ì£¼ìš” ê¸°ëŠ¥

YouTube ëŒ“ê¸€ ì „ì²˜ë¦¬ ë° ë°ì´í„° ì •ì œ

comments_labeled_for_training.csv ìƒì„± ë° ê²€ìˆ˜

BERT ê¸°ë°˜ sentiment classifier fine-tuning

í•™ìŠµëœ ëª¨ë¸(bert_sentiment/) ì €ì¥

ìƒˆë¡œìš´ ëŒ“ê¸€ì— ëŒ€í•œ inference ì‹¤í–‰

#### ê³¼ê±° ê¸°ëŠ¥ (í˜„ì¬ëŠ” ì£¼ì„ ì²˜ë¦¬ë¨)

classical baseline(baseline_model.py) ì‹¤í–‰

active learning í›„ë³´ ì¶”ì¶œ(sentiment_utils.py)

# 2. Classical Baseline & Data Pipeline
## baseline_model.py -> active learningìš©ìœ¼ë¡œ ì‚¬ìš© í˜„ì¬ ë…¸íŠ¸ë¶ì—ì„  ì‚¬ìš©ì•ˆí•¨(ì£¼ì„ì²˜ë¦¬) BERT ëª¨ë¸ì´ ì£¼ëª¨ë¸ì„

classical ML baseline ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ìë™ ì‹¤í–‰í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

### ì£¼ìš” ì—­í• 
#### ğŸ”¹ ë¼ë²¨ ë³‘í•© (merge_all_manual_golds)

ê¸°ì¡´ weak label + manual_label round íŒŒì¼ ë³‘í•©

ì¤‘ë³µ comment â†’ ìµœì‹  ë¼ë²¨ ìš°ì„ 

weak ë°ì´í„°ëŠ” weak_frac ë¹„ìœ¨ë¡œ downsample í›„ ë³‘í•©

ìµœì¢… ì¶œë ¥: comments_labeled_merged.csv

#### ğŸ”¹ baseline ëª¨ë¸ í•™ìŠµ (baseline_once)

TF-IDF(word/char) + neg lexicon feature + Logistic Regression

valid set ê¸°ë°˜ macro-F1 ìµœì  hyperparam + tau(neutral threshold) íƒìƒ‰

ìµœì¢… ëª¨ë¸ ì €ì¥: artifacts/classic/

#### ğŸ”¹ ì˜µì…˜: inference + n-gram ë¶„ì„ (optional_outputs)

ìƒˆë¡œìš´ CSVì— ëŒ€í•œ batch prediction

chi-square ê¸°ë°˜ top n-gram export

í˜„ì¬ í”„ë¡œì íŠ¸ì—ì„œì˜ ìœ„ì¹˜

classical baseline / active learning ìš©ë„ì˜ ë ˆê±°ì‹œ íŒŒì´í”„ë¼ì¸

BERT fine-tuning ì´í›„ë¡œëŠ” ë¹„êµìš© ë˜ëŠ” ë³´ì¡° ë°ì´í„° ìƒì„±ì— ì‚¬ìš©

# 3. Shared Utilities / Classic Model Logic
## sentiment_utils.py

ë°ì´í„° ì²˜ë¦¬, feature ìƒì„±, classical ëª¨ë¸ êµ¬ì„±, active learning, inferenceê¹Œì§€
ì „ë°˜ì„ ë‹´ë‹¹í•˜ëŠ” ê³µìš© ìœ í‹¸ë¦¬í‹° íŒŒì¼ì…ë‹ˆë‹¤.

### ğŸ”¹ Lexicon & Feature
NEG_LEXICON

í•œêµ­ì–´ + ì˜ì–´ ë¶€ì • í‘œí˜„ ì‚¬ì „

YouTube ìš•ì„¤/ë¹„í•˜/ë¹„íŒ í‘œí˜„ ë‹¤ìˆ˜ í¬í•¨
(ì˜ˆ: trash, garbage, sucks, cringe, disgusting, worst, waste of time, clickbait â€¦)

neg_lexicon_features(texts)

lexicon í¬í•¨ ì—¬ë¶€ë¥¼ 0/1 sparse matrixë¡œ ë³€í™˜

TF-IDFì™€ í•¨ê»˜ ë¶€ì • ì‹ í˜¸ ê°•í™” featureë¡œ ì‚¬ìš©

### ğŸ”¹ Data Helpers
clean(t)

URL, @mention ì œê±°

ê³µë°± ì •ë¦¬ ë“± ê¸°ë³¸ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬

load_dataset(path)

CSV ë¡œë“œ í›„ text, label ì •ì œ

label set ê²€ì¦

split_train_valid_test

stratified ë°©ì‹ train/valid/test ë¶„ë¦¬

### ğŸ”¹ Classic Model: ClassicSentiment

TF-IDF(word) + TF-IDF(char) + lexicon feature â†’ hstack

LogisticRegression(saga) + CalibratedClassifierCV ì‚¬ìš©

ì œê³µ ë©”ì„œë“œ:

fit

predict

predict_proba

save(out_dir)

load(out_dir)

### ğŸ”¹ Neutral Policy & Grid Search
apply_neutral_policy

max proba < tau â†’ ê°•ì œ neutral

pos/neg ì°¨ì´ ì‘ìœ¼ë©´ ë” í° ìª½ìœ¼ë¡œ ì¬í• ë‹¹

small_grid_search

(C, l1_ratio, tau) ì¡°í•© íƒìƒ‰

valid ê¸°ë°˜ macro-F1 ìµœì  config ì„ íƒ

### ğŸ”¹ Active Learning
select_active_learning_candidates

ê¸°ì¡´ baseline ëª¨ë¸ë¡œ low-confidence ìƒ˜í”Œ ì„ ë³„

SVD + KMeans clusterë¡œ ê·¸ë£¹í™”

clusterë³„ ëŒ€í‘œ ambiguous ìƒ˜í”Œ ì„ íƒ

### ğŸ”¹ Batch Inference & N-gram ë¶„ì„
batch_predict

ìƒˆë¡œìš´ CSVì— ëŒ€í•´ pred, p_neg, p_neu, p_pos ì¶”ê°€

neutral policy ì ìš©

ê²°ê³¼ CSV ì €ì¥

chi2_top_ngrams_from_df

labelë³„ íŠ¹ì§•ì  n-gram ì¶”ì¶œ

chi-square score ê¸°ë°˜ ranking í›„ CSV export

# 4. Data Files
## comments_labeled_for_training.csv

BERT fine-tuningì— ì‚¬ìš©ëœ ìµœì¢… ë¼ë²¨ë§ ë°ì´í„°ì…‹

í¬í•¨ ì»¬ëŸ¼:

text

label (neg, neu, pos)

í•„ìš” ì‹œ metadata (video_id, comment_id, â€¦)

comments_for_inference.csv

BERT ëª¨ë¸ë¡œ ê°ì •ì„ ì˜ˆì¸¡í•  raw ëŒ“ê¸€ ë°ì´í„°

ë…¸íŠ¸ë¶ì—ì„œ batch inference ìˆ˜í–‰ì— ì‚¬ìš©ë¨
